{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_mp0c0k8rCS"
   },
   "source": [
    "# Training a PIX2PIX Model using PyTorch / ONNX\n",
    "\n",
    "This notebook walks you through the steps of training your own image-to-image machine learning model.\n",
    "\n",
    "Basically all you have to do is put your cursor in a cell and press Shift+Enter. At the end, you can download the latest model from the `output` folder (it will be called something like `generator_epoch_XXX.onnx`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqhpNeaL8z7C",
    "outputId": "bd15bb05-4640-4dd2-c43c-08fd8f0c70e7"
   },
   "outputs": [],
   "source": [
    "# Make sure you are connected to a runtime with a GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading your images\n",
    "Once the cell above has executed, you're connected to a runtime (a machine in the cloud). Click the folder icon in the left sidebar, then drag your ZIP file with files on the sidebar to upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFa-5ik3_MFq",
    "outputId": "f5eeabe8-246d-49f4-cced-1ad32e0b5b02"
   },
   "outputs": [],
   "source": [
    "# Install ONNX (not installed by default)\n",
    "#import locale\n",
    "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "%pip install -q onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcftxzAmuAzP"
   },
   "outputs": [],
   "source": [
    "# Import all other dependencies\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJijQKrV9J93",
    "outputId": "fb4fc26a-ce98-4baf-c5c9-397f5ce09d2f"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(\"GPU is\", \"available\" if gpu_available else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tO0EYyUMkCe",
    "outputId": "478c04ac-4edf-41fc-c6fc-0a5964a9a399"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: If you don't have a dataset yet, you can download a pre-existing dataset by executing this cell.\n",
    "# If you have a dataset already, you can skip this step.\n",
    "!curl -O https://algorithmicgaze.s3.amazonaws.com/datasets/faces_contour.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the ZIP file and put it in the correct folder\n",
    "!mkdir -p datasets/faces\n",
    "!unzip -j -o -qq *.zip -d datasets/faces\n",
    "# Remove macOS metadata cruft\n",
    "!rm -rf datasets/faces/._*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-1v5IkJ-Nex"
   },
   "outputs": [],
   "source": [
    "# Some helper functions for creating/checking directories.\n",
    "def directory_should_exist(*args):\n",
    "    dir = os.path.join(*args)\n",
    "    if not os.path.isdir(dir):\n",
    "        raise Exception(\"Path '{}' is not a directory.\".format(dir))\n",
    "    return dir\n",
    "\n",
    "def ensure_directory(*args):\n",
    "    dir = os.path.join(*args)\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N2O3py7u-JC"
   },
   "outputs": [],
   "source": [
    "input_dir = directory_should_exist(\"datasets/faces\")\n",
    "output_dir = ensure_directory(\"output\")\n",
    "sample_interval = 300\n",
    "snapshot_interval = 1\n",
    "epochs = 100\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJuVKp5GujGO"
   },
   "outputs": [],
   "source": [
    "# Create the dataset class\n",
    "class Pix2PixDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, jitter_size=60):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.jitter_size = jitter_size\n",
    "        self.image_files = [\n",
    "            f for f in os.listdir(root_dir) if f.endswith(\".jpg\") or f.endswith(\".png\")\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def resize_image(self, input_image, target_image, width, height):\n",
    "        input_image = TF.resize(input_image, [height, width], interpolation=TF.InterpolationMode.BICUBIC)\n",
    "        target_image = TF.resize(target_image, [height, width], interpolation=TF.InterpolationMode.BICUBIC)\n",
    "        return input_image, target_image\n",
    "\n",
    "    def random_crop(self, input_image, target_image, crop_height, crop_width):\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(input_image, output_size=(crop_height, crop_width))\n",
    "        input_image = TF.crop(input_image, i, j, h, w)\n",
    "        target_image = TF.crop(target_image, i, j, h, w)\n",
    "        return input_image, target_image\n",
    "\n",
    "    def random_jitter(self, input_image, target_image):\n",
    "        # Resize to slightly larger image\n",
    "        w, h = input_image.size\n",
    "        input_image, target_image = self.resize_image(input_image, target_image, w + self.jitter_size, h + self.jitter_size)\n",
    "        \n",
    "        # Random crop back to original size\n",
    "        input_image, target_image = self.random_crop(input_image, target_image, h, w)\n",
    "        \n",
    "        # Random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            input_image = TF.hflip(input_image)\n",
    "            target_image = TF.hflip(target_image)\n",
    "        \n",
    "        return input_image, target_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        # Convert to RGB if necessary\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(mode=\"RGB\")\n",
    "\n",
    "        # Split the image into input and target\n",
    "        w, h = image.size\n",
    "        target_image = image.crop((0, 0, w // 2, h))\n",
    "        input_image = image.crop((w // 2, 0, w, h))\n",
    "\n",
    "        # Apply random jitter\n",
    "        input_image, target_image = self.random_jitter(input_image, target_image)\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "        return input_image, target_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTRhTxTUNd-d"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = Pix2PixDataset(input_dir, transform=transform)\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "z1CkVmZINZo8",
    "outputId": "5856e7ec-827d-499a-bcfa-5c0e2f16c880"
   },
   "outputs": [],
   "source": [
    "# Show a single image from the dataset\n",
    "def plot_image(subplot, title, img):\n",
    "    img = (img + 1) / 2\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "    subplot.imshow(img)\n",
    "    subplot.set_title(title)\n",
    "    subplot.axis(\"off\")\n",
    "\n",
    "input_img, target_img = next(iter(dataloader))\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plot_image(ax1, \"Input Image\", input_img[0])\n",
    "plot_image(ax2, \"Target Image\", target_img[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0S8t5BySulh3"
   },
   "outputs": [],
   "source": [
    "# Implement the UNet architecture for the generator\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, bn=True, dropout=False):\n",
    "        super(UNetBlock, self).__init__()\n",
    "        self.conv = (\n",
    "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels) if bn else None\n",
    "        self.dropout = nn.Dropout(0.5) if dropout else None\n",
    "        self.act = nn.LeakyReLU(0.2) if down else nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn:\n",
    "            x = self.bn(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        return self.act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFmjby3Tum_G"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.down1 = UNetBlock(3, 64, down=True, bn=False)\n",
    "        self.down2 = UNetBlock(64, 128)\n",
    "        self.down3 = UNetBlock(128, 256)\n",
    "        self.down4 = UNetBlock(256, 512)\n",
    "        self.down5 = UNetBlock(512, 512)\n",
    "        self.down6 = UNetBlock(512, 512)\n",
    "        self.down7 = UNetBlock(512, 512)\n",
    "        self.down8 = UNetBlock(512, 512, bn=False)\n",
    "\n",
    "        self.up1 = UNetBlock(512, 512, down=False, dropout=True)\n",
    "        self.up2 = UNetBlock(1024, 512, down=False, dropout=True)\n",
    "        self.up3 = UNetBlock(1024, 512, down=False, dropout=True)\n",
    "        self.up4 = UNetBlock(1024, 512, down=False)\n",
    "        self.up5 = UNetBlock(1024, 256, down=False)\n",
    "        self.up6 = UNetBlock(512, 128, down=False)\n",
    "        self.up7 = UNetBlock(256, 64, down=False)\n",
    "\n",
    "        self.final = nn.Sequential(nn.ConvTranspose2d(128, 3, 4, 2, 1), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        u1 = self.up1(d8)\n",
    "        u2 = self.up2(torch.cat([u1, d7], 1))\n",
    "        u3 = self.up3(torch.cat([u2, d6], 1))\n",
    "        u4 = self.up4(torch.cat([u3, d5], 1))\n",
    "        u5 = self.up5(torch.cat([u4, d4], 1))\n",
    "        u6 = self.up6(torch.cat([u5, d3], 1))\n",
    "        u7 = self.up7(torch.cat([u6, d2], 1))\n",
    "        return self.final(torch.cat([u7, d1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XoK7HKXuofx"
   },
   "outputs": [],
   "source": [
    "# Implement the discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            UNetBlock(6, 64, bn=False),\n",
    "            UNetBlock(64, 128),\n",
    "            UNetBlock(128, 256),\n",
    "            UNetBlock(256, 512),\n",
    "            nn.Conv2d(512, 1, 4, 1, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.model(torch.cat([x, y], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuhzmZFquqEg"
   },
   "outputs": [],
   "source": [
    "# Define the loss functions and optimizers\n",
    "criterion_gan = nn.BCELoss()\n",
    "criterion_pixel = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "968aaAGLurrj"
   },
   "outputs": [],
   "source": [
    "# Load snapshot if available\n",
    "def get_latest_snapshot(output_dir):\n",
    "    snapshots = glob.glob(os.path.join(output_dir, \"snapshot_epoch_*.pth\"))\n",
    "    if not snapshots:\n",
    "        return None\n",
    "    return max(snapshots, key=os.path.getctime)\n",
    "\n",
    "def get_latest_generator(output_dir):\n",
    "    generators = glob.glob(os.path.join(output_dir, \"generator_epoch_*.onnx\"))\n",
    "    if not generators:\n",
    "        return None\n",
    "    return max(generators, key=os.path.getctime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqrPl_uVuuEc"
   },
   "outputs": [],
   "source": [
    "def load_snapshot(generator, discriminator, g_optimizer, d_optimizer, snapshot_path):\n",
    "    checkpoint = torch.load(snapshot_path, map_location=device, weights_only=False)\n",
    "    generator.load_state_dict(checkpoint[\"generator\"])\n",
    "    discriminator.load_state_dict(checkpoint[\"discriminator\"])\n",
    "    g_optimizer.load_state_dict(checkpoint[\"g_optimizer\"])\n",
    "    d_optimizer.load_state_dict(checkpoint[\"d_optimizer\"])\n",
    "    start_epoch = int(os.path.basename(snapshot_path).split(\"_\")[2].split(\".\")[0])\n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GO3efseEuvlZ"
   },
   "outputs": [],
   "source": [
    "# Create the training loop\n",
    "def train(generator, discriminator, dataloader, opts):\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    # Get fixed input/output for visualization\n",
    "    fixed_set = next(iter(dataloader))\n",
    "    fixed_input = fixed_set[0][0].unsqueeze(0)\n",
    "    fixed_target = fixed_set[1][0].unsqueeze(0)\n",
    "    # fixed_input = next(iter(dataloader))[0][0].unsqueeze(0)  # Get a fixed input for visualization\n",
    "\n",
    "    start_epoch = 1\n",
    "    if not opts.restart:\n",
    "        latest_snapshot = get_latest_snapshot(opts.output_dir)\n",
    "        if latest_snapshot:\n",
    "            latest_epoch = load_snapshot(\n",
    "                generator, discriminator, g_optimizer, d_optimizer, latest_snapshot\n",
    "            )\n",
    "            # The last epoch was\n",
    "            start_epoch = latest_epoch + 1\n",
    "            print(f\"Resuming training from epoch {latest_epoch}\")\n",
    "        else:\n",
    "            print(\"No snapshots found. Starting from scratch.\")\n",
    "    else:\n",
    "        print(\"Restarting training from scratch.\")\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + opts.epochs):\n",
    "        for i, (input_img, target_img) in enumerate(tqdm(dataloader)):\n",
    "            input_img = input_img.to(device)\n",
    "            target_img = target_img.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            fake_img = generator(input_img)\n",
    "            d_real = discriminator(input_img, target_img)\n",
    "            d_fake = discriminator(input_img, fake_img.detach())\n",
    "            d_loss_real = criterion_gan(d_real, torch.ones_like(d_real))\n",
    "            d_loss_fake = criterion_gan(d_fake, torch.zeros_like(d_fake))\n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            fake_img = generator(input_img)\n",
    "            d_fake = discriminator(input_img, fake_img)\n",
    "            g_loss_gan = criterion_gan(d_fake, torch.ones_like(d_fake))\n",
    "            g_loss_pixel = criterion_pixel(fake_img, target_img) * 100\n",
    "            g_loss = g_loss_gan + g_loss_pixel\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            if i % opts.sample_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    fake_img = generator(fixed_input.to(device))\n",
    "                    img_sample = torch.cat(\n",
    "                        (fixed_input.cpu(), fake_img.cpu(), fixed_target.cpu()), -1\n",
    "                    )\n",
    "                    img_sample = (img_sample + 1) / 2\n",
    "                    np_img = img_sample.squeeze(0).numpy().transpose((1, 2, 0))\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"Epoch {epoch}\")\n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    plt.imshow(np_img)\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                    image_path = f\"{opts.output_dir}/epoch_{epoch}_iter_{i}.jpg\"\n",
    "                    save_image(\n",
    "                        img_sample,\n",
    "                        image_path,\n",
    "                        nrow=3,\n",
    "                        normalize=True,\n",
    "                    )\n",
    "\n",
    "        if (epoch + 1) % opts.snapshot_interval == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"generator\": generator.state_dict(),\n",
    "                    \"discriminator\": discriminator.state_dict(),\n",
    "                    \"g_optimizer\": g_optimizer.state_dict(),\n",
    "                    \"d_optimizer\": d_optimizer.state_dict(),\n",
    "                },\n",
    "                f\"{opts.output_dir}/snapshot_epoch_{epoch}.pth\",\n",
    "            )\n",
    "\n",
    "            # Save to ONNX format\n",
    "            onnx_path = f\"{opts.output_dir}/generator_epoch_{epoch}.onnx\"\n",
    "            generator.eval()\n",
    "            dummy_input = torch.randn(1, 3, 512, 512).to(device)\n",
    "            traced_script_module = torch.jit.trace(generator, dummy_input)\n",
    "            torch.onnx.export(\n",
    "                traced_script_module,\n",
    "                dummy_input,\n",
    "                onnx_path,\n",
    "                export_params=True,\n",
    "                opset_version=11,\n",
    "                do_constant_folding=True,\n",
    "                input_names=[\"input\"],\n",
    "                output_names=[\"output\"],\n",
    "                dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "            )\n",
    "            print(f\"ONNX model exported to {onnx_path}\")\n",
    "            generator.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "2QUHdooiuztD",
    "outputId": "a26f1c1a-794b-4cfc-9ef8-0dff8dbd83a0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "opts = {\n",
    "    \"output_dir\": output_dir,\n",
    "    \"sample_interval\": sample_interval,\n",
    "    \"snapshot_interval\": snapshot_interval,\n",
    "    \"epochs\": epochs,\n",
    "    \"restart\": False,\n",
    "}\n",
    "train(generator, discriminator, dataloader, SimpleNamespace(**opts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4IWPRe7YESG"
   },
   "source": [
    "## Optional: Copy the generator model to Google Drive\n",
    "\n",
    "Downloading the model from the sidebar can be very slow. You can save some time downloading/uploading the generator by using Google Drive. In the next steps we'll connect to Google Drive and upload the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6kTEBQCYHzu",
    "outputId": "93dcf344-754f-4ef6-bf38-89d8632333f8"
   },
   "outputs": [],
   "source": [
    "# Step 1: Mount Google Drive. This will ask for permissions.\n",
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "-K_jwLSyTiS2",
    "outputId": "cc7d5f07-4b88-4eb4-ecbb-aba80ae4df11"
   },
   "outputs": [],
   "source": [
    "# Step 2: Copy the latest .onnx file to Google Drive\n",
    "import shutil\n",
    "drive_folder = '/drive/MyDrive/2024-raive'\n",
    "ensure_directory(drive_folder)\n",
    "shutil.copy(get_latest_generator(output_dir), drive_folder)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
